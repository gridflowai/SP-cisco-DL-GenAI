{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d27c30-c426-4e22-b44a-5712b94278f8",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "Tokenization: To demonstrate how a piece of text is split into individual tokens according to the rules of a pre-trained Transformer model's tokenizer. This helps in understanding how text is processed before being fed into the model.\n",
    "\n",
    "Token IDs Conversion: To show how these tokens are mapped to their corresponding numerical IDs, which are used by the model for computation. This step is crucial as models work with numerical data rather than raw text.\n",
    "\n",
    "Encoding: To illustrate how the entire text is converted into a format that can be directly used by the model, including creating tensors that represent the token IDs.\n",
    "\n",
    "Decoding: To demonstrate how numerical token IDs can be converted back into human-readable text, which helps in interpreting the model's outputs.\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5367b2d8-5af4-419a-bda6-0b008e9722e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d08fac-ec1c-4409-b4fa-a6aae02f78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          cache_dir=r'D:\\AI-DATASETS\\07-Hugging-Face-Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819823cc-1474-49df-bc40-6e33a4199259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text to tokenize\n",
    "text = \"Hello, how are you doing today, unforgettable, undesirable, Chat-Masala, dosa?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696d0d75-d34b-429d-a355-d163833884bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', ',', 'how', 'are', 'you', 'doing', 'today', ',', 'un', '##for', '##get', '##table', ',', 'und', '##es', '##ira', '##ble', ',', 'chat', '-', 'mas', '##ala', ',', 'dos', '##a', '?']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d8bf47-8b16-45f6-aa40-d4571d54910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [7592, 1010, 2129, 2024, 2017, 2725, 2651, 1010, 4895, 29278, 18150, 10880, 1010, 6151, 2229, 7895, 3468, 1010, 11834, 1011, 16137, 7911, 1010, 9998, 2050, 1029]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26a0e48-c85b-445a-839a-a0848420ece6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings: {'input_ids': tensor([[  101,  7592,  1010,  2129,  2024,  2017,  2725,  2651,  1010,  4895,\n",
      "         29278, 18150, 10880,  1010,  6151,  2229,  7895,  3468,  1010, 11834,\n",
      "          1011, 16137,  7911,  1010,  9998,  2050,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Convert text to token IDs directly\n",
    "encodings = tokenizer(text, return_tensors='pt')\n",
    "print(\"Encodings:\", encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2caf847f-261e-419a-87dd-69fc9119a36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: [CLS] hello, how are you doing today, unforgettable, undesirable, chat - masala, dosa? [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Decode token IDs back to text\n",
    "decoded_text = tokenizer.decode(encodings['input_ids'][0])\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4d714-38b3-4591-9979-c78a6999e729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
